{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ap1TzJZzhQm4"
   },
   "source": [
    "# 1-Importamos Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "n3poTF1FhNXM",
    "outputId": "fe4b36af-042c-446a-d304-90ee6e2575cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yt_gaYYHehZR"
   },
   "source": [
    "# 2-Cargamos los datos csv locales descargados de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBZBNKqyeiCJ"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Dataset/train.csv')\n",
    "df_test = pd.read_csv('Dataset/test.csv')\n",
    "df_Sample_Subm = pd.read_csv('Dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "87jME07bi31u",
    "outputId": "a8fded78-bbec-4f34-fa3b-d5b4d6d6a5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4) (3263, 2)\n"
     ]
    }
   ],
   "source": [
    "print (df_train.shape, df_test.shape, df_Sample_Subm.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "NQ7ibtUWhaXM",
    "outputId": "e678a7b2-a16e-4d33-c32a-d96d464bde95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "YmenbLOpjDzH",
    "outputId": "5341a70d-ce8d-4ed5-8048-ba6a20ecc157"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "uiunZcDujXeg",
    "outputId": "6044671b-73d7-405c-82ac-f22675198809",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0\n",
       "5  12       0\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sample_Subm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1-Colocamos las 'keyword' dentro de la columna 'cleaned_text': \n",
    "\n",
    "Y la colocamos ANTES del texto del tweet ya si lo colocariamos después del texto serían recortados por la función cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['keyword'].notnull() == False,'cleaned_text'] = df_train['text'] \n",
    "df_train.loc[df_train['keyword'].notnull() == True,'cleaned_text'] = df_train['keyword'] + ' ' + df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[df_test['keyword'].notnull() == False,'cleaned_text'] = df_test['text'] \n",
    "df_test.loc[df_test['keyword'].notnull() == True,'cleaned_text'] = df_test['keyword'] + ' ' + df_test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2-Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #Librería de Python para usar expresiones regulares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_line(text):\n",
    "    text = re.sub(r'\\t', ' ', text) # Ekiminamos tabs. \n",
    "    text = re.sub(r'\\n', ' ', text) # Eliminamos los \"enters\".\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url(text): #Función que utilizamos el clean(text)\n",
    "    #Eliminamos varios twetts que estan truncados como \"Experts in France begin examining airplane debris found on Reunion Island: French air \n",
    "        #accident experts o... http://t.co/YVVPznZmXg #news\".\n",
    "    text = re.sub(r' \\w{1,3}\\.{3,3} http\\S{0,}', ' ', text)\n",
    "    text = re.sub(r' \\w{1,3}Û_ http\\S{0,}', ' ', text)\n",
    "    #Algunas palabras (como via) o simbolos antes de 'htttp' son eliminados, asumimos que estas palabras/simbolos no tienen un\n",
    "    #significado semantico y no influyen en su posicion. \n",
    "    text = re.sub(r\"mp3 http\\S{0,}\", r\" \", text)\n",
    "    text = re.sub(r\"rar http\\S{0,}\", r\" \", text)\n",
    "    pattern = re.compile(r'( pin\\:\\d+ | via )http\\S{0,}')\n",
    "    text = pattern.sub(r' ', text)\n",
    "    #Eliminamos otros patrones que no tinen gran significado en los tweets: \n",
    "    pattern = re.compile(r'Full read by|Full read b|Full read|Full rea|Full re|Full r')\n",
    "    text = pattern.sub(r' ', text)\n",
    "    pattern = re.compile(r'Full story at|Full story a|Full story|Full stor|Full sto|Full st|Full s')\n",
    "    text = pattern.sub(r' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):    \n",
    "    text = new_line(text)                              # Eliminamos tabs y enters. \n",
    "    text = re.sub(r'(&amp;|&gt;|&lt;)', \" \", text)     # Eliminamos algunos patrones.\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                   # Eliminamos espacios extras.\n",
    "    text = url(text)                                   # Eliminamos las urls y algunos patrones.\n",
    "    \n",
    "    # Eliminamos los patrones \" 'translated' as 'USER' \" y \" 'traslated' as \"USER_NAME\" y múltiples \"USER\":\n",
    "        #https://www.kaggle.com/quentinsarrazin/tweets-preprocessing   #Idea sacada de este Notebook.\n",
    "        #https://arxiv.org/ftp/arxiv/papers/1807/1807.07752.pdf        #Paper donde hicieron esto y obtuvieron mejores resultados.\n",
    "    text = re.sub(r'@\\S{0,}', ' USER ', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Eliminamos espacios extra.   \n",
    "    text = re.sub(r'\\b(USER)( \\1\\b)+', r'\\1', text)\n",
    "    \n",
    "    #Hay multiples \"letras\" que se repiten como \"ooooohhhh\", y las truncamos a 2 letras. NO hacemos un truncate de 1 letra\n",
    "    #ya que podría generar falsos significados: por ej. si truncamos 'good' nos diría 'god':\n",
    "    text = re.sub(r'([a-zA-Z])\\1{1,}', r'\\1\\1', text)\n",
    "    \n",
    "    #  Eliminamos URLs si no fueron todavía eliminadas por la función URL: \n",
    "    text = re.sub(r\"htt\\S{0,}\", \" \", text)\n",
    "    \n",
    "    #Removemos todos los caracteres que NO esten en la lista [a-zA-Z\\d\\s]\n",
    "    text = re.sub(r\"[^a-zA-Z\\d\\s]\", \" \", text)\n",
    "    \n",
    "    # Hacemos lo mismo que los patrones 'traslated as USER' pero ahora para digitos: \" 'translated' to 'NUMBER' \" y múltiples \"NUMBER NUMBER... to NUMBER\":\n",
    "    text = re.sub(r'^\\d\\S{0,}| \\d\\S{0,}| \\d\\S{0,}$', ' NUMBER ', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Eliminamos espacios extra. \n",
    "    text = re.sub(r'\\b(NUMBER)( \\1\\b)+', r'\\1', text)\n",
    "    \n",
    "    # Removemos digitos que NO fueron eliminados arriba en 'NUMBER translation'\n",
    "    text = re.sub(r\"[0-9]\", \" \", text)\n",
    "    \n",
    "    text = text.strip()                          # Eliminamos espacios al principio y al fin del string.\n",
    "    text = re.sub(r' via\\s{1,}USER$', ' ', text) # Eliminamos los ' via USER'.\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Removemos espacios extra. \n",
    "    text = text.strip()              # Removemos espacios al principio y al final del string. \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos las funciones a la columna cleaned_text:\n",
    "df_train.cleaned_text = df_train.cleaned_text.apply(clean)\n",
    "df_test.cleaned_text = df_test.cleaned_text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the stage of text processing:\n",
      "...the size of longest text string in train set is  171\n",
      "...the size of longest text string in test set is  161\n"
     ]
    }
   ],
   "source": [
    "max_length_tr = df_train.cleaned_text.map(len).max()\n",
    "max_length_te = df_test.cleaned_text.map(len).max()\n",
    "max_length = max(max_length_tr, max_length_te)\n",
    "\n",
    "#Imprimimos el mayor texto de tweet en train y en test:\n",
    "print(\"At the stage of text processing:\")\n",
    "print(f\"...the size of longest text string in train set is  {max_length_tr}\")\n",
    "print(f\"...the size of longest text string in test set is  {max_length_te}\")\n",
    "#Vemos que tiene 10 caracteres más que la versión anterior (Ya que agregamos la keyword al inicio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After we cut tails of the longest tweets:\n",
      "...the size of longest text string in train set is  146\n",
      "...the size of longest text string in test set is  146\n"
     ]
    }
   ],
   "source": [
    "# Vamos a cortar los tweets y ahora nuestro MAX posible será (max_length - delta). Delta lo definimos como 25.\n",
    "#Esto lo hacemos ya que al final del tweet hay menor importancia en los mismos (generalmente dicen 'by user' o alguna \n",
    "#página web o algún dato sin relevancia, lo importante está al inicio). \n",
    "def cut(max_len, delta, x):\n",
    "    new_max = max_len - delta\n",
    "    length = len(x)\n",
    "    if length <= new_max:\n",
    "        return x \n",
    "    else:\n",
    "        return x[:(new_max-length)]\n",
    "\n",
    "delta = 25 \n",
    "df_train.text = df_train.text.map(lambda x: cut(max_length, delta, x))\n",
    "df_test.text = df_test.text.map(lambda x: cut(max_length, delta, x))\n",
    "\n",
    "new_max_length_tr = df_train.text.map(len).max()\n",
    "new_max_length_te = df_test.text.map(len).max()\n",
    "\n",
    "#Ahora imprimimos nuevamente y vemos que como máximo tenemos 137:\n",
    "print(\"After we cut tails of the longest tweets:\")\n",
    "print(f\"...the size of longest text string in train set is  {new_max_length_tr}\")\n",
    "print(f\"...the size of longest text string in test set is  {new_max_length_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRXc3E8YlPC-"
   },
   "source": [
    "# 4-Aproximación mediante BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1kLvBG1lR0p"
   },
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de deep learning desarrollado por Google de código abierto. Es utilizado por muchos investigadores e industrias para para resolver muchas tareas de NLP. \n",
    "\n",
    "Ktrain (https://github.com/amaiya/ktrain) es un contenedor (wrapper) ligero para la biblioteca de deeplearning TensorFlow Keras (https://www.tensorflow.org/guide/keras/sequential_model) para ayudar a construir, entrenar e implementar ANN's y otros modelos de ML. Diseñado para hacer que el aprendizaje profundo (deep learning) y la IA sean más accesibles y fáciles de aplicar.\n",
    "\n",
    "Ktrain proporciona soporte para la aplicación de muchas arquitecturas de aprendizaje profundo pre-entrenadas en el dominio de NLP; y BERT es una de ellas. Para resolver este problema, utilizaremos la implementación del BERT pre-entrenado proporcionado por ktrain y lo afinaremos/tunearemos para clasificar si los tweets del desastre son reales o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPuc7-ZTnH8l"
   },
   "source": [
    "SOLO estamos interesados en la columna TEXTO y TARGET. Las cuales usaremos para clasificar nuestros Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqtOYxNHnje7"
   },
   "source": [
    "## Importamos las librerias para leer el csv de entrenamiento (train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLEzScXLnSfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "using Keras version: 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leeremos train.csv y realizaremos una división en la columna 'target' donde definiremos el 20% de los datos como el conjunto de validación (validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuestro train.csv está en el DF 'df_train'\n",
    "random_seed = 12342\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_train['cleaned_text'], df_train['target'], shuffle=True, test_size = 0.2, random_state=random_seed, stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TK2ZDBisXh5"
   },
   "source": [
    "\n",
    "## Convertimos la data en features para BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktrain proporciona una feature muy interesante que nos permite convertir directamente la data de tipo texto en feautures que el modelo necesita. Todo el preprocesamiento del texto no se necesita hacer manualmente... sino que la libreria se encarga de esto. Luego de leer nuestra data de pandas utilizaremos la funciòn 'text_from_array'.\n",
    "\n",
    "Esta funciòn lo que harà es descargar el modelo de BERT pre-entrenado y su vocabulario. Y en 'preprocess_mode' especificamos 'bert' ya que de esta manera el texto se preprocesarà de una manera especìfica para BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train_bert,  y_train_bert), (x_val_bert, y_val_bert), preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n",
    "                                                                                         x_test = x_val, y_test=y_val,\n",
    "                                                                                          class_names= [\"0\", \"1\"],\n",
    "                                                                                          preprocess_mode='bert',\n",
    "                                                                                          lang = 'en',\n",
    "                                                                                          maxlen=65, \n",
    "                                                                                          max_features=35000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos BERT en un objeto 'learner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La 1ra funciòn ('text_classifier') carga el modelo pre-entrenado de BERT con una capa Dense final inicializada aleatoriamente. Ya que todas las capas del modelo son entrenables, los pesos de todas las capas del modelo se actualizarán durante el proceso de backpropagation.\n",
    "\n",
    "La 2da funciòn ('get_learner') crea un objeto 'learner' con data de entrenamiento y data de validaciòn que son usados para \"afinar\" el clasificador. EL ùltimo paràmetro de 'get_learner' es el \"batch size\" (usamos un batch size pequeño, de 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 65\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert', train_data=(x_train_bert, y_train_bert), preproc=preproc)\n",
    "learner = ktrain.get_learner(model, train_data=(x_train_bert, y_train_bert), val_data=(x_val_bert, y_val_bert), batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO (\"tuneando\" el Clasificador BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo, 1ro buscaremos el learning rate òptimo que funcione para nuestro problema. Ktrain provee un mètodo 'lr_find' que nos permite entrenar al modelo con diferentes learning rates y plotear el loss del modelo a medida que el LR incrementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find()    #SImulamos un entrenamiento para encontrar el mejor LR.\n",
    "#learner.lr_plot()    #Para observar el plot del LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la prueba anterior obtuvimos que el clasificador provee un loss mìnimo cuando el LR es 1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a usar dicho LR para entrenar el modelo mediante el mètodo 'autofit'. Este mètodo entrena el clasificador y automàticamente selecciona la mejor performance del modelo previniendo el underfitting y overfitting del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 1e-05...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/1024\n",
      "6090/6090 [==============================] - 163s 27ms/sample - loss: 0.4894 - acc: 0.7798 - val_loss: 0.3820 - val_acc: 0.8424\n",
      "Epoch 2/1024\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.3810 - acc: 0.8455 - val_loss: 0.3771 - val_acc: 0.8450\n",
      "Epoch 3/1024\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.3319 - acc: 0.8716 - val_loss: 0.3874 - val_acc: 0.8418\n",
      "Epoch 4/1024\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.2872 - acc: 0.8924\n",
      "Epoch 00004: Reducing Max LR on Plateau: new max lr will be 5e-06 (if not early_stopping).\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.2872 - acc: 0.8924 - val_loss: 0.3939 - val_acc: 0.8345\n",
      "Epoch 5/1024\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.2381 - acc: 0.9151 - val_loss: 0.4151 - val_acc: 0.8372\n",
      "Epoch 6/1024\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.2053 - acc: 0.9274\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 2.5e-06 (if not early_stopping).\n",
      "6090/6090 [==============================] - 165s 27ms/sample - loss: 0.2053 - acc: 0.9274 - val_loss: 0.4502 - val_acc: 0.8313\n",
      "Epoch 7/1024\n",
      "6090/6090 [==============================] - ETA: 0s - loss: 0.1731 - acc: 0.9420Restoring model weights from the end of the best epoch.\n",
      "6090/6090 [==============================] - 166s 27ms/sample - loss: 0.1731 - acc: 0.9420 - val_loss: 0.4771 - val_acc: 0.8253\n",
      "Epoch 00007: early stopping\n",
      "Weights from best epoch have been loaded into model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ac0367f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.autofit(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Disaster       0.86      0.88      0.87       869\n",
      "    Disaster       0.83      0.80      0.82       654\n",
      "\n",
      "    accuracy                           0.85      1523\n",
      "   macro avg       0.84      0.84      0.84      1523\n",
      "weighted avg       0.84      0.85      0.84      1523\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[762, 107],\n",
       "       [129, 525]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate(val_data=(x_val_bert, y_val_bert), class_names=['No Disaster', 'Disaster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos buenos f1-score para cada clase predicha. Luego de entrenar nuestro modelo con el mètodo 'autofit', ahora es tiempo de realizar predicciones sobre la data de TEST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenemos la variable predictora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable predictora es obtenida pasandole el modelo y el objeto 'preproc' al mètodo 'get_predictor'. Este 'predictor' puede ser usado para realizar predicciones en nuestra data de TEST directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.print_layers()  #Para observar todas las capas del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Guardamos nuestro modelo y sus pesos para su futuro uso:\n",
    "learner.model.save_weights(\"model-bert-more-cleaning.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYcANcCEshUL"
   },
   "source": [
    "## 5-Predecimos en el CSV de TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVdCi9HNsj4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test[\"target\"] = predictor.predict(df_test[\"cleaned_text\"].tolist())\n",
    "df_test = df_test[[\"id\", \"target\"]]\n",
    "df_test.to_csv(\"submission_bert_more_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP2-BERT_COLAB.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
