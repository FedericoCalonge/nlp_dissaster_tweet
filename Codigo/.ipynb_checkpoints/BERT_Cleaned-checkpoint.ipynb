{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH0AO7C9eBEP"
   },
   "source": [
    "# 1-Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWswCrSweLbQ"
   },
   "source": [
    "Basado en: https://medium.com/analytics-vidhya/finetuning-bert-using-ktrain-for-disaster-tweets-classification-18f64a50910b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4koPLXdUeDio"
   },
   "source": [
    "## Objetivos\n",
    "Participar en la competencia de Kaggle 'Real or Not', donde se deben utilizar los datos de Tweets que nos brinda Kaggle en 2 archivos CSVs. Debemos clasificar los Tweets que hablan sobre desastres naturales contra los que NO hablan de estos (Y generalmente hablan de los mismos \"metafóricamente\"). \n",
    "\n",
    "Link set de datos y competencia: https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "De esta manera, como dijimos previamente, debemos identificar y clasificar si los tweets corresponden o no a tweets que hablan sobre catástrofes. Tenemos un dataset 'train' con una columna 'target' donde \"etiquetamos\" cuales son verdaderos (1) o falsos (0). Identificar esto es una tarea compleja debido a la ambigüedad en la estructura lingüística de los tweets y, por lo tanto, no siempre está claro si las palabras de una persona realmente están anunciando un desastre o no. Por ejemplo, si una persona tuitea:\n",
    "“On the plus side look at the sky last night, it was ablaze” (En español: \n",
    "\"En el lado positivo, miré el cielo anoche, estaba en llamas\"). \n",
    "La expresión 'ablaze' no significa que está en llamas realmente, sino que es una metáfora indicando que el cielo está anaranjado. Para nosotros es fácil entenderlo, pero para las máquinas no lo es. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ap1TzJZzhQm4"
   },
   "source": [
    "## Importamos Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "n3poTF1FhNXM",
    "outputId": "fe4b36af-042c-446a-d304-90ee6e2575cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yt_gaYYHehZR"
   },
   "source": [
    "# 2-Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Etphmj0ek5w"
   },
   "source": [
    "## Cargamos los datos csv locales descargados de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBZBNKqyeiCJ"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_Sample_Subm = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Bnn7J05iqx9"
   },
   "source": [
    "## Exploración mínima de los datos (la exploración completa la hicimos en el TP1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtQQpVh1mhzy"
   },
   "source": [
    "Nuestras Columnas del dataset son:\n",
    " - id: Identificador único de cada tweet\n",
    " - keyword: Una palabra clave particular de cada tweet (puede ser NaN)\n",
    " - location - El lugar donde fue emitido el tweet (puede ser NaN)\n",
    " - text: texto del tweet\n",
    " - target: Si el tweet trata acerca de un desastre real, el valor es 1, sino 0  (solo en train.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "87jME07bi31u",
    "outputId": "a8fded78-bbec-4f34-fa3b-d5b4d6d6a5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4) (3263, 2)\n"
     ]
    }
   ],
   "source": [
    "print (df_train.shape, df_test.shape, df_Sample_Subm.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "NQ7ibtUWhaXM",
    "outputId": "e678a7b2-a16e-4d33-c32a-d96d464bde95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  ...                                               text target\n",
       "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
       "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
       "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
       "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
       "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
       "5   8     NaN  ...  #RockyFire Update => California Hwy. 20 closed...      1\n",
       "6  10     NaN  ...  #flood #disaster Heavy rain causes flash flood...      1\n",
       "7  13     NaN  ...  I'm on top of the hill and I can see a fire in...      1\n",
       "8  14     NaN  ...  There's an emergency evacuation happening now ...      1\n",
       "9  15     NaN  ...  I'm afraid that the tornado is coming to our a...      1\n",
       "\n",
       "[10 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "YmenbLOpjDzH",
    "outputId": "5341a70d-ce8d-4ed5-8048-ba6a20ecc157"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "uiunZcDujXeg",
    "outputId": "6044671b-73d7-405c-82ac-f22675198809"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0\n",
       "5  12       0\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sample_Subm.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRXc3E8YlPC-"
   },
   "source": [
    "# 3-Aproximación mediante BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1kLvBG1lR0p"
   },
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de deep learning desarrollado por Google de código abierto. Es utilizado por muchos investigadores e industrias para para resolver muchas tareas de NLP. \n",
    "\n",
    "Ktrain (https://github.com/amaiya/ktrain) es un contenedor (wrapper) ligero para la biblioteca de deeplearning TensorFlow Keras (https://www.tensorflow.org/guide/keras/sequential_model) para ayudar a construir, entrenar e implementar ANN's y otros modelos de ML. Diseñado para hacer que el aprendizaje profundo (deep learning) y la IA sean más accesibles y fáciles de aplicar.\n",
    "\n",
    "Ktrain proporciona soporte para la aplicación de muchas arquitecturas de aprendizaje profundo pre-entrenadas en el dominio de NLP; y BERT es una de ellas. Para resolver este problema, utilizaremos la implementación del BERT pre-entrenado proporcionado por ktrain y lo afinaremos/tunearemos para clasificar si los tweets del desastre son reales o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPuc7-ZTnH8l"
   },
   "source": [
    "SOLO estamos interesados en la columna TEXTO y TARGET. Las cuales usaremos para clasificar nuestros Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqtOYxNHnje7"
   },
   "source": [
    "## Importamos las librerias para leer el csv de entrenamiento (train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLEzScXLnSfO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFbBDX2Jnvdt"
   },
   "source": [
    "Leeremos train.csv y realizaremos una división en la columna 'target' donde definiremos el 20% de los datos como el conjunto de validación (validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErubcZinoF6y"
   },
   "outputs": [],
   "source": [
    "#Nuestro train.csv está en el DF 'df_train'\n",
    "random_seed = 12342\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_train['text'], df_train['target'], shuffle=True, test_size = 0.2, random_state=random_seed, stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiPmFFrvoVeZ"
   },
   "source": [
    "## Convertimos la data en features para BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9WYO6x8oaMK"
   },
   "source": [
    "ktrain proporciona una feature muy interesante que nos permite convertir directamente la data de tipo texto en feautures que el modelo necesita. Todo el preprocesamiento del texto no se necesita hacer manualmente... sino que la libreria se encarga de esto. Luego de leer nuestra data de pandas utilizaremos la funciòn 'text_from_array'.\n",
    "\n",
    "Esta funciòn lo que harà es descargar el modelo de BERT pre-entrenado y su vocabulario. Y en 'preprocess_mode' especificamos 'bert' ya que de esta manera el texto se preprocesarà de una manera especìfica para BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "HVnG_or1opp9",
    "outputId": "4ad7437b-a0a0-400b-9668-6a9efd09028a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train_bert,  y_train_bert), (x_val_bert, y_val_bert), preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n",
    "                                                                                         x_test = x_val, y_test=y_val,\n",
    "                                                                                          class_names= [\"0\", \"1\"],\n",
    "                                                                                          preprocess_mode='bert',\n",
    "                                                                                          lang = 'en',\n",
    "                                                                                          maxlen=65, \n",
    "                                                                                          max_features=35000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G21Hi4nnrGFK"
   },
   "source": [
    "## Cargamos BERT en un objeto 'learner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wp81NWNZrv4i"
   },
   "source": [
    "La 1ra funciòn ('text_classifier') carga el modelo pre-entrenado de BERT con una capa Dense final inicializada aleatoriamente. Ya que todas las capas del modelo son entrenables, los pesos de todas las capas del modelo se actualizarán durante el proceso de backpropagation.\n",
    "\n",
    "La 2da funciòn ('get_learner') crea un objeto 'learner' con data de entrenamiento y data de validaciòn que son usados para \"afinar\" el clasificador. EL ùltimo paràmetro de 'get_learner' es el \"batch size\" (usamos un batch size pequeño, de 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "_wJXjWcJrBf1",
    "outputId": "0a820800-338a-4be1-c761-860e64c2fa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 65\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('bert', train_data=(x_train_bert, y_train_bert), preproc=preproc)\n",
    "learner = ktrain.get_learner(model, train_data=(x_train_bert, y_train_bert), val_data=(x_val_bert, y_val_bert), batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwcBgy9ps5_T"
   },
   "source": [
    "## ENTRENAMIENTO (\"tuneando\" el Clasificador BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30Eu19m0tBy1"
   },
   "source": [
    "Para entrenar el modelo, 1ro buscaremos el learning rate òptimo que funcione para nuestro problema. Ktrain provee un mètodo 'lr_find' que nos permite entrenar al modelo con diferentes learning rates y plotear el loss del modelo a medida que el LR incrementa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "EaDvHAr5oxU8",
    "outputId": "33fbf995-bd79-4685-92cb-636b6f3cec24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Train on 6090 samples\n",
      "Epoch 1/1024\n",
      " 128/6090 [..............................] - ETA: 1:17:36 - loss: 0.8209 - acc: 0.3750"
     ]
    }
   ],
   "source": [
    "learner.lr_find()    #SImulamos un entrenamiento para encontrar el mejor LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUXtXyIhr2oT"
   },
   "source": [
    "## Ploteamos el learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "nxKMTQVzsE3y",
    "outputId": "42229ada-c664-44d9-9445-138f29cf25be"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Para observar el plot del LR:\n",
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU1xTfz2sB6Z"
   },
   "source": [
    "Podemos observar que el clasificador provee un loss mìnimo cuando el LR es 1e-5. ENtonces, usamos este LR para entrenar el modelo mediante el mètodo 'autofit'. Este mètodo entrena el clasificador y automàticamente selecciona la mejor performance del modelo previniendo el underfitting y overfitting del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "LTfYhhpFrUcE",
    "outputId": "77ab6de0-3c0d-4c04-a2e8-f33fa8939513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 1e-05...\n",
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/1024\n",
      "  16/6090 [..............................] - ETA: 1:26:53 - loss: 0.9178 - acc: 0.3125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f8c61bf13f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mautofit\u001b[0;34m(self, lr, epochs, early_stopping, reduce_on_plateau, reduce_factor, cycle_momentum, monitor, checkpoint_folder, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    778\u001b[0m         hist = self.fit(lr, epochs, early_stopping=early_stopping,\n\u001b[1;32m    779\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    902\u001b[0m                                  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                                  \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                                  callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3580\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3581\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3582\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.autofit(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2AFlSy8sXCq"
   },
   "outputs": [],
   "source": [
    "learner.validate(val_data=(x_val_bert, y_val_bert), class_names=['No Disaster', 'Disaster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTL2lZ8Hthec"
   },
   "source": [
    "Ahora somos capaces de tener una precisiòn de validaciòn del 84% con un buen F-1 Score para cada una de las clases predichas. \n",
    "Luego de entrenar nuestro modelo con el mètodo 'autofit', ahora es tiempo de realizar predicciones sobre la data de TEST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TK2ZDBisXh5"
   },
   "source": [
    "\n",
    "## Obtenemos la variable predictora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETTaFA3htp-k"
   },
   "source": [
    "La variable predictora es obtenida pasandole el modelo y el objeto 'preproc' al mètodo 'get_predictor'. Este 'predictor' puede ser usado para realizar predicciones en nuestra data de TEST directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgnHeBHBscnD"
   },
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72H4yYlOse7S"
   },
   "outputs": [],
   "source": [
    "learner.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYcANcCEshUL"
   },
   "source": [
    "## Predecimos en el CSV de TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVdCi9HNsj4e"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df[\"target\"] = predictor.predict(test_df[\"text\"].tolist())\n",
    "\n",
    "test_df = test_df[[\"id\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTOse3O2sn91"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84MBdhxgsqmj"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(\"submission_bert_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKF0paWft2kY"
   },
   "source": [
    "## Subimos las predicciones de TEST a Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXLe_NRtuBBy"
   },
   "source": [
    "Como último paso subimos nuestras predicciones a Kaggle y chequeamos el SCORE obtenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ePjC3VmuQM9"
   },
   "source": [
    "VER.... Logramos una precisión del ......83.4% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsuqinTvua4A"
   },
   "source": [
    "# 3- Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aHtbwJuud3a"
   },
   "source": [
    "Utilizamos las features de Ktrain para implementar de una manera sencilla el complejo modelo de BERT. AL final fuimos capaces de lograr una precisioǹ en TEST de .........\n",
    "\n",
    "Uno de los mayores problema con BERT es que toma mucho tiempo entrenando. Para mejorar esto, podemos aplicar una versiòn màs ligera de BERT como distilBERT. Tambien, para reducir el tiempo de entrenamiento, los pesos de todas las capas pueden ser congeladas (frozen) a excepciòn de la capa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmyKVrjGudFh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP2-BERT_COLAB.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
